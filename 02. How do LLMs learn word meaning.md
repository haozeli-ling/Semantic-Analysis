## How do LLMs learn word meaning

Before diving into our computational model for meaning, let's take a quick glimpse at the algorithms behind Large Language Models (LLMs). Then, we will compare the LLM approach with the one we are going to design, so you can decide which offers a more intuitive understanding of language meaning.

### Big Question

How can a machine learn the meaning of words without a dictionary?

Although large language models can use words in remarkably fluent and appropriate ways, they do not recognize word meaning in the same way that human beings do.

- For humans, word meaning is grounded in $\color{red}{\text{perception, action, and lived experience}}$ : we learn what ''apple'' means by seeing, touching, tasting, and interacting with apples in the world.
- An LLM never perceives or interacts with the world directly. It learns entirely from $\color{red}{\text{textual patterns}}$ , discovering how words co-occur with other words across massive datasets.

As a result, an LLM’s ''understanding'' of a word consists in a statistical representation of its linguistic contexts, not in a concept tied to sensory experience, intentions, or beliefs. In this sense, LLMs model how words are used, not what words are.

### Core idea 

''The meaning of a word is its use in the language.'' --- [Ludwig Wittgenstein](https://en.wikipedia.org/wiki/Ludwig_Wittgenstein)

Let’s define the meaning of a word by its distribution in language use – its neighboring words or grammatical environments. 

> Suppose you encounter an unfamiliar word in a corpus: <br>
>
> (1) The $\color{blue}{\text{flirple}}$ barked loudly and chased mailman. <br>
> (2) A $\color{blue}{\text{flirple}}$ wagged its tail happily when its owner came home. <br>
> (3) My neighbor’s $\color{blue}{\text{flirple}}$ bit my shoe this morning. <br>
> 
> Can you guess the meaning of this word without a dictionary? 

Computers do not understand words directly; text must first be transformed into numbers that a machine can process. This approach is actually inspired by observations of human linguistic behavior. 

A classic demonstration that word meaning can be analyzed along systematic dimensions comes from [The Measurement of Meaning (1957)](https://gwern.net/doc/psychology/1957-osgood-themeasurementofmeaning.pdf) by Osgood, Suci, and Tannenbaum. Using the semantic differential method, they asked participants to rate words and concepts on pairs of opposite adjectives (such as ''good–bad'', ''strong–weak'', ''active–passive''). Their large-scale empirical studies showed that, across many languages and domains, people’s judgments of word meaning consistently clustered along three major affective dimensions: 

- valence: the pleasantness of the stimulus
- arousal: the intensity of emotion provoked by the stimulus
- dominance: the degree of control exerted by the stimulus

Human subjects are were recruited to rate words along these dimensions

|                   | **Word**                                                                 | **Score**                                                                  | **Word** | **Score** |
|-----------------------------|----------------------------------------------------------------|----------------------------------------------| ------ | ----- |
| **Valence**      |  ''love''               | 1.000   | ''toxic'' | 0.008 |
| **Arousal**      |  ''elated''  | 0.960 | ''napping'' | 0.046 |
| **Dominance** | ''powerful'' | 0.991 | ''weak'' | 0.045 | 

This finding suggested that, beyond dictionary definitions, words carry structured affective meanings that can be measured $\color{red}{\text{quantitatively}}$ , providing early evidence that semantic knowledge has an underlying dimensional organization rather than being an unstructured collection of discrete symbols.

Based on the distribution of a word in texts, we can represent each word as a list of numbers, called a $\color{red}{\text{word embedding}}$ . These numbers are learned automatically from text. Let's see a toy example to get an intuitive understanding of this mechanism. 

For each word we took a single instance from a corpus, and we show the 4 word window from that instance: 

Why embeddings matter 

- Words with similar meanings end up close together
- Words with different meanings end up far apart

As a result, meaning is represented as a point in multi-dimentional space on distribution. This kind of semantic arithmetic illustrates that word embeddings encode not just individual meanings but also relations between meanings. An LLM can learn the relation among words implicitly in the vector geometry. 

### How are embeddings learned? 

Embeddings are learned from large text corpora. A classic method, $\color{red}{\text{Word2Vec}}$ , trains a simple neural network on a task of predicting words from their context (or vice versa) and uses the internal weights as the word vectors.

LLMs are traind with a very simple task: $\color{red}{\text{Guess the next word}}$

> ''The cat is sleeping on the ___ ''
> The model adjusts word vectors so that: likely words get higher probability, while unlikely words get lower probability.

Over billions of examoples, the model learns: 

- which words tend to appear together
- which words can replace each other
- which words signal similar situations

In doing so, the model adjusts the word vectors such that they can successfully reflect the context and those vectors end up encoding meaningful relationships.









